{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:08:11.694406Z",
     "start_time": "2019-09-20T08:08:10.245384Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ta\n",
    "# from fastai import *\n",
    "# from fastai.tabular import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from rolling import RollingWindowSplit\n",
    "from sklearn.metrics import r2_score as r2d2\n",
    "from joblib import dump, load\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style = \"whitegrid\")\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:08:11.701389Z",
     "start_time": "2019-09-20T08:08:11.698385Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# path = 'D://Coding//XTX Forecasting Challenge//data-training.csv'\n",
    "# df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:08:14.047385Z",
     "start_time": "2019-09-20T08:08:11.705386Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'D://Coding//XTX Forecasting Challenge//data-training.file'\n",
    "df = pd.read_feather(path, use_threads=8)\n",
    "df = df.astype('float32')\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:08:14.060388Z",
     "start_time": "2019-09-20T08:08:14.052388Z"
    }
   },
   "outputs": [],
   "source": [
    "askRateList = ['askRate' + str(i) for i in range(0,15)]\n",
    "askSizeList = ['askSize' + str(i) for i in range(0,15)]\n",
    "bidRateList = ['bidRate' + str(i) for i in range(0,15)]\n",
    "bidSizeList = ['bidSize' + str(i) for i in range(0,15)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T03:01:58.506630Z",
     "start_time": "2019-09-18T03:01:58.500630Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Figuring out what [y] is\n",
    "# # y(t) is midRate(t+87) - midRate(t), clipped to (-5.5)\n",
    "# df['expectedY'] = df.midRate.diff(87).shift(-87).clip(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-sectional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T04:49:23.759120Z",
     "start_time": "2019-09-20T04:49:23.746115Z"
    }
   },
   "outputs": [],
   "source": [
    "# different from submission\n",
    "def compute_cross_sectional(df):\n",
    "    # Cross-sectional features\n",
    "    df['spread'] = df.askRate0 - df.bidRate0\n",
    "    df['midRate'] = (df.askRate0 + df.bidRate0) / 2\n",
    "    df['bidAskVol'] = df.askSize0 + df.bidSize0\n",
    "    df['totalBidVol1'] = df.bidSize0 + df.bidSize1\n",
    "    df['totalAskVol1'] = df.askSize0 + df.askSize1\n",
    "    for i in range(2,15):\n",
    "        df['totalBidVol' + str(i)] = df['totalBidVol' + str(i-1)] + df['bidSize' + str(i)]\n",
    "        df['totalAskVol' + str(i)] = df['totalAskVol' + str(i-1)] + df['askSize' + str(i)]\n",
    "    for i in range(1,15):\n",
    "        df['bidAskRatio' + str(i)] = df['totalBidVol' + str(i)] / df['totalAskVol' + str(i)]\n",
    "    df['totalAvailVol'] = df.totalBidVol14 + df.totalAskVol14\n",
    "    df['vwaBid'] = np.einsum('ij,ji->i', df[bidRateList], df[bidSizeList].T) / df[bidSizeList].sum(axis=1)\n",
    "    df['vwaAsk'] = np.einsum('ij,ji->i', df[askRateList], df[askSizeList].T) / df[askSizeList].sum(axis=1)\n",
    "    df['vwaBidDMid'] = df.midRate - df.vwaBid\n",
    "    df['vwaAskDMid'] = df.vwaAsk - df.midRate\n",
    "    df['diff_vwaBidAskDMid'] = df.vwaAskDMid - df.vwaBidDMid\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T04:48:34.013551Z",
     "start_time": "2019-09-20T04:48:34.003572Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    b1, a1 = (df.bidRate0 < df.bidRate0.shift(1)), (df.askRate0 < df.askRate0.shift(1))\n",
    "    b2, a2 = (df.bidRate0 == df.bidRate0.shift(1)), (df.askRate0 == df.askRate0.shift(1))\n",
    "    valsB, valsA = [0, (df.bidSize0 - df.bidSize0.shift(1))], [0, (df.askSize0 - df.askSize0.shift(1))]\n",
    "    defaultB, defaultA = df.bidSize0, df.askSize0\n",
    "    df.fillna(0, inplace=True)\n",
    "    df['deltaVBid'] = np.select([b1,b2], valsB, default=defaultB)\n",
    "    df['deltaVAsk'] = np.select([a1,a2], valsA, default=defaultA)\n",
    "    df['VOI'] = df.deltaVBid - df.deltaVAsk\n",
    "    df['OIR'] = (df.bidSize0 - df.askSize0)/(df.bidSize0 + df.askSize0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual time features â€” can consider adding more to the lags list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T04:48:35.235134Z",
     "start_time": "2019-09-20T04:48:35.229132Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_manual_time_features(df):\n",
    "    lags = [*np.arange(1,10), *np.arange(10,100,10), *np.arange(100,1000,100)]\n",
    "    def addTimeFeatures(i):\n",
    "        df['daskRate' + str(i)] = df.askRate0.diff(i)\n",
    "        df['dbidRate' + str(i)] = df.bidRate0.diff(i)\n",
    "    for i in lags:\n",
    "        addTimeFeatures(i)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T04:49:37.995489Z",
     "start_time": "2019-09-20T04:49:25.625486Z"
    }
   },
   "outputs": [],
   "source": [
    "df = compute_cross_sectional(df)\n",
    "df = add_time_features(df)\n",
    "df = add_manual_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T04:49:51.526232Z",
     "start_time": "2019-09-20T04:49:46.406872Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.to_feather('intermediate.file')\n",
    "df = pd.read_feather('intermediate.file', use_threads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tick chart version with ffill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T04:50:07.001386Z",
     "start_time": "2019-09-20T04:50:06.644402Z"
    }
   },
   "outputs": [],
   "source": [
    "# midrate version\n",
    "df['time'] = pd.date_range(start='1/1/1970', periods=2999999, freq='T')\n",
    "df.set_index('time', inplace=True)\n",
    "df_mid = df.midRate.resample('15Min').ohlc()\n",
    "df_mid['vol'] = df.bidAskVol.resample('15Min').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T03:06:35.616471Z",
     "start_time": "2019-09-18T03:02:24.330407Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 5 min\n",
    "df_mid_ta = ta.add_all_ta_features(df_mid, \"open\", \"high\", \"low\", \"close\", \"vol\", fillna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T03:15:55.257617Z",
     "start_time": "2019-09-19T03:15:54.716303Z"
    }
   },
   "outputs": [],
   "source": [
    "# dump(df_mid_ta, 'df_mid_ta.joblib')\n",
    "df_mid_ta = load('df_mid_ta.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T03:07:17.706451Z",
     "start_time": "2019-09-18T03:06:35.912455Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 30s\n",
    "new_df = df.join(df_mid_ta).ffill().astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T08:55:15.867149Z",
     "start_time": "2019-09-19T08:55:01.350433Z"
    }
   },
   "outputs": [],
   "source": [
    "# dump(new_df, 'new_df.joblib')\n",
    "new_df = load('new_df.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:08:18.988237Z",
     "start_time": "2019-09-20T08:08:18.967271Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_limited_features_orig(df):\n",
    "    df['midRate'] = (df.askRate0 + df.bidRate0) / 2\n",
    "    df['totalBidVol1'] = df.bidSize0 + df.bidSize1\n",
    "    df['totalAskVol1'] = df.askSize0 + df.askSize1\n",
    "    df['bidAskVol'] = df.askSize0 + df.bidSize0\n",
    "    for i in range(2,5):\n",
    "        df['totalBidVol' + str(i)] = df['totalBidVol' + str(i-1)] + df['bidSize' + str(i)]\n",
    "        df['totalAskVol' + str(i)] = df['totalAskVol' + str(i-1)] + df['askSize' + str(i)]    \n",
    "    df['bidAskRatio4'] = df['totalBidVol' + str(4)] / df['totalAskVol' + str(4)]\n",
    "    df['OIR'] = (df.bidSize0 - df.askSize0)/(df.bidSize0 + df.askSize0)\n",
    "    def addTimeFeatures(i):\n",
    "        df['daskRate' + str(i)] = df.askRate0.diff(i)\n",
    "        df['dbidRate' + str(i)] = df.bidRate0.diff(i)\n",
    "    for i in range(6,11):\n",
    "        addTimeFeatures(i)\n",
    "    df['time'] = pd.date_range(start='1/1/1970', periods=len(df), freq='T')\n",
    "    df.set_index('time', inplace=True)\n",
    "    df_mid = df.midRate.resample('15Min').ohlc()\n",
    "    df_mid['vol'] = df.bidAskVol.resample('15Min').mean()\n",
    "    df_mid['volume_adi'] = ta.volume.acc_dist_index(df_mid.high, df_mid.low, df_mid.close, df_mid.vol, fillna=True)\n",
    "    df_mid['others_dlr'] = ta.others.daily_log_return(df_mid.close, fillna=True)\n",
    "    df = df.join(df_mid[['volume_adi', 'others_dlr']]).ffill().astype('float32')\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:08:28.714943Z",
     "start_time": "2019-09-20T08:08:24.042943Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparkle = create_limited_features_orig(df); sparkle;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limited sparkle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we predict volume_adi from the other features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:08:30.882381Z",
     "start_time": "2019-09-20T08:08:29.550361Z"
    }
   },
   "outputs": [],
   "source": [
    "X = sparkle.drop([*sparkle.columns[:71], 'volume_adi'], axis=1).values\n",
    "y = sparkle.volume_adi.values\n",
    "\n",
    "# standardise\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T05:21:42.948415Z",
     "start_time": "2019-09-20T05:21:34.400417Z"
    }
   },
   "outputs": [],
   "source": [
    "lasso_adi = LassoLarsCV(cv=rlcv, n_jobs=-1).fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T05:21:42.963415Z",
     "start_time": "2019-09-20T05:21:42.951413Z"
    }
   },
   "outputs": [],
   "source": [
    "def rlcvscore_adi(model):\n",
    "    cvtrain, cvvalid, cvvalidsig = [], [], []\n",
    "    for inc, (train_index, valid_index) in enumerate(rlcv.split(X_scaled), 1):\n",
    "        x_train, x_valid = X_scaled[train_index], X_scaled[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        cvtrain.append(model.score(x_train, y_train))\n",
    "        cvvalid.append(model.score(x_valid, y_valid))\n",
    "        sigmoid = (1/(1+np.exp(-0.22*model.predict(x_valid)))-0.5)*20  \n",
    "        cvvalidsig.append(r2d2(y_valid, sigmoid))\n",
    "    print(f'{np.array(cvtrain).round(4)}')\n",
    "    print(f'{np.array(cvvalid).round(4)}')\n",
    "    print(f'{np.array(cvvalidsig).round(4)}')\n",
    "    print(f'{np.mean(cvtrain):.4f}, {np.mean(cvvalid):.4f}, {np.mean(cvvalidsig):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T05:21:44.466413Z",
     "start_time": "2019-09-20T05:21:42.967414Z"
    }
   },
   "outputs": [],
   "source": [
    "rlcvscore_adi(lasso_adi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T05:22:21.281108Z",
     "start_time": "2019-09-20T05:22:21.276107Z"
    }
   },
   "outputs": [],
   "source": [
    "lasso_adi.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T05:22:55.323811Z",
     "start_time": "2019-09-20T05:22:55.026814Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_adi = X_scaled @ lasso_adi.coef_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T05:23:21.838712Z",
     "start_time": "2019-09-20T05:23:21.374715Z"
    }
   },
   "outputs": [],
   "source": [
    "sparkle.volume_adi = preds_adi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T05:27:24.196495Z",
     "start_time": "2019-09-20T05:27:23.939497Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparkle_lim = create_limited_features(df.head(45).copy()); sparkle_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:05:42.047197Z",
     "start_time": "2019-09-20T08:05:41.872198Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparkle_lim = create_limited_features(df.head(60).copy()); sparkle_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T04:57:51.904266Z",
     "start_time": "2019-09-20T04:57:51.889239Z"
    }
   },
   "outputs": [],
   "source": [
    "ta.volume.acc_dist_index??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T05:26:21.729771Z",
     "start_time": "2019-09-20T05:26:21.618772Z"
    }
   },
   "outputs": [],
   "source": [
    "ta.others.daily_log_return??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:09:03.122715Z",
     "start_time": "2019-09-20T08:09:03.118751Z"
    }
   },
   "outputs": [],
   "source": [
    "rlcv = RollingWindowSplit(n_splits=5, compatible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:15:45.082346Z",
     "start_time": "2019-09-20T08:15:44.930344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bidAskRatio4', 'OIR', 'daskRate6', 'dbidRate6', 'daskRate7',\n",
       "       'dbidRate7', 'daskRate8', 'dbidRate8', 'daskRate9', 'dbidRate9',\n",
       "       'daskRate10', 'dbidRate10', 'volume_adi', 'others_dlr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkle.drop(sparkle.columns[:71], axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:09:06.781448Z",
     "start_time": "2019-09-20T08:09:05.282476Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 40s\n",
    "# undropped\n",
    "X = sparkle.drop(sparkle.columns[:71], axis=1).values\n",
    "y = df.y.values\n",
    "\n",
    "# standardise\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:09:48.035054Z",
     "start_time": "2019-09-20T08:09:48.028053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_limited.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(scaler, 'scaler_limited.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:09:20.000976Z",
     "start_time": "2019-09-20T08:09:08.894020Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 11s on limited variables, 1 min on pca variables, 16m21s on 232 non-pca variables\n",
    "lasso = LassoLarsCV(cv=rlcv, n_jobs=-1).fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually the lasso above has seen the entire dataset...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:09:20.018977Z",
     "start_time": "2019-09-20T08:09:20.003980Z"
    }
   },
   "outputs": [],
   "source": [
    "def rlcvscore(model):\n",
    "    cvtrain, cvvalid, cvvalidsig = [], [], []\n",
    "    for inc, (train_index, valid_index) in enumerate(rlcv.split(X_scaled), 1):\n",
    "        x_train, x_valid = X_scaled[train_index], X_scaled[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        cvtrain.append(model.score(x_train, y_train))\n",
    "        cvvalid.append(model.score(x_valid, y_valid))\n",
    "        sigmoid = (1/(1+np.exp(-0.22*model.predict(x_valid)))-0.5)*20  \n",
    "        cvvalidsig.append(r2d2(y_valid, sigmoid))\n",
    "    print(f'{np.array(cvtrain).round(4)}')\n",
    "    print(f'{np.array(cvvalid).round(4)}')\n",
    "    print(f'{np.array(cvvalidsig).round(4)}')\n",
    "    print(f'{np.mean(cvtrain):.4f}, {np.mean(cvvalid):.4f}, {np.mean(cvvalidsig):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:09:21.618979Z",
     "start_time": "2019-09-20T08:09:20.022979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0604 0.061  0.0628 0.0589 0.0653]\n",
      "[0.061  0.0628 0.0589 0.0653 0.066 ]\n",
      "[0.0608 0.0626 0.058  0.0652 0.0653]\n",
      "0.0617, 0.0628, 0.0624\n"
     ]
    }
   ],
   "source": [
    "rlcvscore(lasso) # limited variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:09:26.036791Z",
     "start_time": "2019-09-20T08:09:26.030790Z"
    }
   },
   "outputs": [],
   "source": [
    "# dump(lasso, 'lasso_limited.joblib')\n",
    "lasso = load('lasso_limited.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:09:27.717772Z",
     "start_time": "2019-09-20T08:09:27.710783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02572668,  0.05097824, -0.01739648, -0.01683773, -0.00606163,\n",
       "       -0.00552953, -0.00564391, -0.00511773, -0.00655901, -0.00624305,\n",
       "       -0.0354066 , -0.03479002,  0.03206284,  0.17627821])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:09:29.376966Z",
     "start_time": "2019-09-20T08:09:28.963960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06203365066030242"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = X_scaled @ lasso.coef_.T\n",
    "r2d2(y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:55:37.150506Z",
     "start_time": "2019-09-19T09:55:37.058535Z"
    }
   },
   "outputs": [],
   "source": [
    "for inc, (train_index, valid_index) in enumerate(rlcv.split(X_scaled),1):\n",
    "    x_train, x_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    print(\"TRAIN:\", (train_index[0], train_index[-1]),\n",
    "          \"VALID:\", (valid_index[0], valid_index[-1]),\n",
    "          \"SIZES:\", (len(x_train), len(x_valid)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:58:29.694653Z",
     "start_time": "2019-09-19T09:58:29.690653Z"
    }
   },
   "outputs": [],
   "source": [
    "params_grid = {'max_depth': np.arange(10),\n",
    "               'min_samples_split': 2*10**np.arange(5),\n",
    "               'min_samples_leaf': 2*10**np.arange(5)\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:58:32.120288Z",
     "start_time": "2019-09-19T09:58:32.115287Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=30, n_jobs=3, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:58:32.691811Z",
     "start_time": "2019-09-19T09:58:32.686834Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_random = RandomizedSearchCV(rf, params_grid, n_iter=30, n_jobs=3, cv=rlcv, random_state=41, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:00:00.932441Z",
     "start_time": "2019-09-19T09:58:34.037439Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_random.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:47:46.631224Z",
     "start_time": "2019-09-19T09:47:46.626222Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=30, max_depth=3, min_samples_split=10, min_samples_leaf=20000,\n",
    "                                 max_features='auto', n_jobs=-1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:47:54.638369Z",
     "start_time": "2019-09-19T09:47:47.984588Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_model.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:47:58.697443Z",
     "start_time": "2019-09-19T09:47:54.641049Z"
    }
   },
   "outputs": [],
   "source": [
    "rlcvscore(rf_model) # realistic cv, max_depth 3, split 10, leaf 10000\n",
    "# [0.0354 0.0389 0.0413 0.0334 0.041 ]\n",
    "# [0.0389 0.0413 0.0334 0.041  0.0275]\n",
    "# [0.0366 0.0392 0.0293 0.0369 0.0202]\n",
    "# 0.0380, 0.0364, 0.0324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T07:12:36.386453Z",
     "start_time": "2019-09-19T07:12:36.353451Z"
    }
   },
   "outputs": [],
   "source": [
    "# for blending validation\n",
    "def get_preds():\n",
    "    # undropped\n",
    "    X = new_df.drop('y', axis=1).values\n",
    "    y = new_df.y.values\n",
    "    scaler = load('scaler.joblib')\n",
    "    X_scaled = scaler.transform(X)\n",
    "    pca = load('pca.joblib')\n",
    "    X_pca = pca.transform(X_scaled)\n",
    "    lasso = load('lassocv.joblib')\n",
    "    \n",
    "    lasso_pred_train, lasso_pred_valid, lasso_pred_validsig = [], [], []\n",
    "    for inc, (train_index, valid_index) in enumerate(rlcv.split(X_pca), 1):\n",
    "        x_train, x_valid = X_pca[train_index], X_pca[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        # obtain lasso predictions\n",
    "        lasso_pred_train.append(lasso.predict(x_train))\n",
    "        lasso_pred_valid.append(lasso.predict(x_valid))\n",
    "        lasso_pred_validsig.append((1/(1+np.exp(-0.22*lasso.predict(x_valid)))-0.5)*20)\n",
    "    return lasso_pred_train, lasso_pred_valid, lasso_pred_validsig    \n",
    "    \n",
    "def get_dropped_preds():\n",
    "    # dropped\n",
    "    X = new_df.drop([*askRateList, *askSizeList, *bidRateList, *bidSizeList, 'y'], axis=1).values\n",
    "    y = new_df.y.values\n",
    "    scaler_drop = load('scaler_drop.joblib')\n",
    "    X_scaled = scaler_drop.transform(X)\n",
    "    pca_drop = load('pca_drop.joblib')\n",
    "    X_pca = pca_drop.transform(X_scaled)\n",
    "#     lasso_drop = load('lassocv_drop.joblib')\n",
    "    lasso_drop = load('lassocv.joblib') # instead use lasso\n",
    "    \n",
    "    y_trainer, y_valider = [], []\n",
    "    lasso_dpred_train, lasso_dpred_valid, lasso_dpred_validsig = [], [], []\n",
    "    for inc, (train_index, valid_index) in enumerate(rlcv.split(X_pca), 1):\n",
    "        x_train, x_valid = X_pca[train_index], X_pca[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]  \n",
    "        # obtain lasso_drop predictions\n",
    "        lasso_dpred_train.append(lasso_drop.predict(x_train))\n",
    "        lasso_dpred_valid.append(lasso_drop.predict(x_valid))\n",
    "        lasso_dpred_validsig.append((1/(1+np.exp(-0.22*lasso_drop.predict(x_valid)))-0.5)*20)\n",
    "        y_trainer.append(y_train)\n",
    "        y_valider.append(y_valid)\n",
    "    return lasso_dpred_train, lasso_dpred_valid, lasso_dpred_validsig, y_trainer, y_valider    \n",
    "    \n",
    "def get_blended_scores():\n",
    "    # average predictions\n",
    "    cvtrain, cvvalid, cvvalidsig = [], [], []\n",
    "    for i in range(5):\n",
    "        train = r2d2(y_trainer[i], (np.array(lasso_dpred_train[i]) + np.array(lasso_pred_train[i]))/2)\n",
    "        valid = r2d2(y_valider[i], (np.array(lasso_dpred_valid[i]) + np.array(lasso_pred_valid[i]))/2)\n",
    "        sigmoid_valid = r2d2(y_valider[i], (np.array(lasso_dpred_validsig[i]) + np.array(lasso_pred_validsig[i]))/2)\n",
    "        cvtrain.append(train)\n",
    "        cvvalid.append(valid)\n",
    "        cvvalidsig.append(sigmoid_valid)   \n",
    "    print(f'{np.array(cvtrain).round(4)}')\n",
    "    print(f'{np.array(cvvalid).round(4)}')\n",
    "    print(f'{np.array(cvvalidsig).round(4)}')\n",
    "    print(f'{np.mean(cvtrain):.4f}, {np.mean(cvvalid):.4f}, {np.mean(cvvalidsig):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T07:10:17.118294Z",
     "start_time": "2019-09-19T07:09:41.953425Z"
    }
   },
   "outputs": [],
   "source": [
    "lasso_pred_train, lasso_pred_valid, lasso_pred_validsig = get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T07:12:52.920189Z",
     "start_time": "2019-09-19T07:12:40.567184Z"
    }
   },
   "outputs": [],
   "source": [
    "lasso_dpred_train, lasso_dpred_valid, lasso_dpred_validsig, y_trainer, y_valider = get_dropped_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T07:12:53.360183Z",
     "start_time": "2019-09-19T07:12:52.924186Z"
    }
   },
   "outputs": [],
   "source": [
    "get_blended_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:34:27.199966Z",
     "start_time": "2019-09-13T08:34:27.194964Z"
    }
   },
   "outputs": [],
   "source": [
    "dep_var = 'y'\n",
    "procs = [FillMissing, Normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:34:46.357867Z",
     "start_time": "2019-09-13T08:34:29.653867Z"
    }
   },
   "outputs": [],
   "source": [
    "path = f'D:\\Coding\\XTX Forecasting Challenge'\n",
    "data = TabularDataBunch.from_df(path = path, df = df[:int(5e5)], dep_var = 'y', procs=procs,\n",
    "                                 valid_idx = list(range(int(4e5),int(5e5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:26:47.618579Z",
     "start_time": "2019-09-13T08:26:17.525620Z"
    }
   },
   "outputs": [],
   "source": [
    "data.show_batch(rows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:27:39.484687Z",
     "start_time": "2019-09-13T06:27:21.410885Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = (TabularList.from_df(df[:int(5e5)], cont_names=df.columns, procs=procs)\n",
    "#                            .split_by_idx(list(range(int(0.8*5e5),int(5e5))))\n",
    "#                            .label_from_df(cols=dep_var, label_cls=FloatList)\n",
    "#                            .databunch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:50:44.246393Z",
     "start_time": "2019-09-13T08:50:44.024206Z"
    }
   },
   "outputs": [],
   "source": [
    "learn = tabular_learner(data, layers=[500,200], metrics=r2_score, ps=[0.001,0.01], emb_drop=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:50:45.534954Z",
     "start_time": "2019-09-13T08:50:45.527952Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:37:03.605733Z",
     "start_time": "2019-09-13T08:36:35.900238Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.lr_find(end_lr=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:37:04.843812Z",
     "start_time": "2019-09-13T08:37:03.608733Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model above has already diverged, we will restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T09:03:36.738452Z",
     "start_time": "2019-09-13T08:50:49.724470Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, 1e-4, wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:41:02.673928Z",
     "start_time": "2019-09-13T08:41:01.868908Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T09:04:03.690828Z",
     "start_time": "2019-09-13T09:04:03.447302Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.save('new_fastai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T09:04:07.023062Z",
     "start_time": "2019-09-13T09:04:06.385055Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T09:05:00.282946Z",
     "start_time": "2019-09-13T09:05:00.098945Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.predict(df.iloc[int(8.1e5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T09:05:02.937494Z",
     "start_time": "2019-09-13T09:05:02.932488Z"
    }
   },
   "outputs": [],
   "source": [
    "df.y.iloc[int(8.1e5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T09:05:38.921082Z",
     "start_time": "2019-09-13T09:05:08.529113Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = learn.get_preds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T07:11:45.848924Z",
     "start_time": "2019-09-20T07:11:45.842924Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_next_data_as_df(iteration):\n",
    "    return pd.DataFrame([df.iloc[iteration][:60].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T07:11:46.082274Z",
     "start_time": "2019-09-20T07:11:46.074274Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_limited_features(df):\n",
    "    df.columns = [*askRateList, *askSizeList, *bidRateList, *bidSizeList]\n",
    "    df['midRate'] = (df.askRate0 + df.bidRate0) / 2\n",
    "    df['totalBidVol1'] = df.bidSize0 + df.bidSize1\n",
    "    df['totalAskVol1'] = df.askSize0 + df.askSize1\n",
    "    df['bidAskVol'] = df.askSize0 + df.bidSize0\n",
    "    for i in range(2,5):\n",
    "        df['totalBidVol' + str(i)] = df['totalBidVol' + str(i-1)] + df['bidSize' + str(i)]\n",
    "        df['totalAskVol' + str(i)] = df['totalAskVol' + str(i-1)] + df['askSize' + str(i)]\n",
    "    df['bidAskRatio4'] = df['totalBidVol' + str(4)] / df['totalAskVol' + str(4)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T07:11:46.305604Z",
     "start_time": "2019-09-20T07:11:46.300603Z"
    }
   },
   "outputs": [],
   "source": [
    "def append_to_df(massive_df, row):\n",
    "    try: row.index = [massive_df.index[-1] + timedelta(minutes=1)]\n",
    "    except IndexError: row.index = [datetime(1970,1,1)]\n",
    "    return massive_df.append(row, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T07:11:47.176847Z",
     "start_time": "2019-09-20T07:11:47.171850Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    df['OIR'] = (df.bidSize0 - df.askSize0)/(df.bidSize0 + df.askSize0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T07:11:47.460739Z",
     "start_time": "2019-09-20T07:11:47.455739Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_manual_time_features(df):\n",
    "    def addTimeFeatures(i):\n",
    "        df['daskRate' + str(i)] = df.askRate0.diff(i)\n",
    "        df['dbidRate' + str(i)] = df.bidRate0.diff(i)\n",
    "    for i in range(6,11):\n",
    "        addTimeFeatures(i)\n",
    "    df.fillna(0, inplace=True) # necessary\n",
    "    return df[-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:04:33.931082Z",
     "start_time": "2019-09-20T08:04:33.917083Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_resample_features(massive_df, resampled_df):\n",
    "    leftovers = (massive_df.index[-1].to_pydatetime().minute+1) % 15\n",
    "    def pad_history():\n",
    "        full_resampled = resampled_df.append(row_ohlcv, sort=False) # (1,5)\n",
    "        a = pd.DataFrame([full_resampled.iloc[0] for j in range(1+1-len(full_resampled))]) # (1,5)\n",
    "        a = a.append(full_resampled, sort=False) # (2,5)\n",
    "        a.index = pd.date_range(start=row_ohlcv.index[-1], periods=len(a), freq='-15Min').sort_values()\n",
    "        full_resampled['volume_adi'] = ta.volume.acc_dist_index(a.high, a.low, a.close, a.vol, fillna=True)\n",
    "        full_resampled['others_dlr'] = ta.others.daily_log_return(a.close, fillna=True)\n",
    "        return full_resampled\n",
    "    if leftovers == 0:\n",
    "        row_ohlcv = massive_df.tail(15).midRate.resample('15Min').ohlc().tail(1)\n",
    "        row_ohlcv['vol'] = massive_df.tail(15).bidAskVol.resample('15Min').mean().tail(1) # row_ohlcv.shape = (1,5)\n",
    "        full_resampled = pad_history()\n",
    "        resampled_df = resampled_df.append(full_resampled, sort=False).tail(2) # when iteration=15, leftovers=0, resampled_df=[]\n",
    "    else:\n",
    "        row_ohlcv = massive_df.tail(leftovers).midRate.resample('15Min').ohlc().tail(1)\n",
    "        row_ohlcv['vol'] = massive_df.tail(leftovers).bidAskVol.resample('15Min').mean().tail(1)\n",
    "        full_resampled = pad_history()\n",
    "    try: massive_df.drop(['volume_adi', 'others_dlr'], axis=1, inplace=True)\n",
    "    except KeyError: pass\n",
    "    massive_df = massive_df.join(full_resampled[['volume_adi', 'others_dlr']])\n",
    "    massive_df = massive_df.ffill().astype('float32')\n",
    "    return massive_df, resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:05:30.994187Z",
     "start_time": "2019-09-20T08:05:30.989185Z"
    }
   },
   "outputs": [],
   "source": [
    "massive_df, resampled_df = pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:05:37.393426Z",
     "start_time": "2019-09-20T08:05:31.619370Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iteration in range(32):\n",
    "    base_row = get_next_data_as_df(iteration)\n",
    "    row = create_limited_features(base_row)\n",
    "    massive_df = append_to_df(massive_df, row)\n",
    "    massive_df = add_time_features(massive_df)\n",
    "    massive_df = add_manual_time_features(massive_df)\n",
    "    massive_df, resampled_df = add_resample_features(massive_df, resampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why does 30 min here have no values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:05:37.569371Z",
     "start_time": "2019-09-20T08:05:37.396372Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "massive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:05:42.047197Z",
     "start_time": "2019-09-20T08:05:41.872198Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparkle_lim = create_limited_features_orig(df.head(132).copy()); sparkle_lim.tail(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
