{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:05:55.165721Z",
     "start_time": "2019-09-19T10:05:52.630723Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ta\n",
    "# from fastai import *\n",
    "# from fastai.tabular import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from rolling import RollingWindowSplit\n",
    "from sklearn.metrics import r2_score as r2d2\n",
    "from joblib import dump, load\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style = \"whitegrid\")\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:33:18.638753Z",
     "start_time": "2019-09-19T09:33:18.634746Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# path = 'D://Coding//XTX Forecasting Challenge//data-training.csv'\n",
    "# df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:06:02.899620Z",
     "start_time": "2019-09-19T10:05:55.167721Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'D://Coding//XTX Forecasting Challenge//data-training.file'\n",
    "df = pd.read_feather(path, use_threads=8)\n",
    "df = df.astype('float32')\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:06:02.910623Z",
     "start_time": "2019-09-19T10:06:02.903625Z"
    }
   },
   "outputs": [],
   "source": [
    "bidSizeList = ['bidSize' + str(i) for i in range(0,15)]\n",
    "askSizeList = ['askSize' + str(i) for i in range(0,15)]\n",
    "bidRateList = ['bidRate' + str(i) for i in range(0,15)]\n",
    "askRateList = ['askRate' + str(i) for i in range(0,15)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T03:01:58.506630Z",
     "start_time": "2019-09-18T03:01:58.500630Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Figuring out what [y] is\n",
    "# # y(t) is midRate(t+87) - midRate(t), clipped to (-5.5)\n",
    "# df['expectedY'] = df.midRate.diff(87).shift(-87).clip(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-sectional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T02:53:49.144651Z",
     "start_time": "2019-09-19T02:53:49.130652Z"
    }
   },
   "outputs": [],
   "source": [
    "# different from submission\n",
    "def compute_cross_sectional(df):\n",
    "    df = pd.DataFrame([base_row])\n",
    "    df.columns = [*askRateList, *askSizeList, *bidRateList, *bidSizeList]\n",
    "\n",
    "    # Cross-sectional features\n",
    "    df['spread'] = df.askRate0 - df.bidRate0\n",
    "    df['midRate'] = (df.askRate0 + df.bidRate0) / 2\n",
    "    df['bidAskVol'] = df.askSize0 + df.bidSize0\n",
    "    df['totalBidVol1'] = df.bidSize0 + df.bidSize1\n",
    "    df['totalAskVol1'] = df.askSize0 + df.askSize1\n",
    "    for i in range(2,15):\n",
    "        df['totalBidVol' + str(i)] = df['totalBidVol' + str(i-1)] + df['bidSize' + str(i)]\n",
    "        df['totalAskVol' + str(i)] = df['totalAskVol' + str(i-1)] + df['askSize' + str(i)]\n",
    "    for i in range(1,15):\n",
    "        df['bidAskRatio' + str(i)] = df['totalBidVol' + str(i)] / df['totalAskVol' + str(i)]\n",
    "    df['totalAvailVol'] = df.totalBidVol14 + df.totalAskVol14\n",
    "    df['vwaBid'] = np.einsum('ij,ji->i', df[bidRateList], df[bidSizeList].T) / df[bidSizeList].sum(axis=1)\n",
    "    df['vwaAsk'] = np.einsum('ij,ji->i', df[askRateList], df[askSizeList].T) / df[askSizeList].sum(axis=1)\n",
    "    df['vwaBidDMid'] = df.midRate - df.vwaBid\n",
    "    df['vwaAskDMid'] = df.vwaAsk - df.midRate\n",
    "    df['diff_vwaBidAskDMid'] = df.vwaAskDMid - df.vwaBidDMid\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T02:53:51.865885Z",
     "start_time": "2019-09-19T02:53:51.856881Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    b1, a1 = (df.bidRate0 < df.bidRate0.shift(1)), (df.askRate0 < df.askRate0.shift(1))\n",
    "    b2, a2 = (df.bidRate0 == df.bidRate0.shift(1)), (df.askRate0 == df.askRate0.shift(1))\n",
    "    valsB, valsA = [0, (df.bidSize0 - df.bidSize0.shift(1))], [0, (df.askSize0 - df.askSize0.shift(1))]\n",
    "    defaultB, defaultA = df.bidSize0, df.askSize0\n",
    "    df.fillna(0, inplace=True)\n",
    "    df['deltaVBid'] = np.select([b1,b2], valsB, default=defaultB)\n",
    "    df['deltaVAsk'] = np.select([a1,a2], valsA, default=defaultA)\n",
    "    df['VOI'] = df.deltaVBid - df.deltaVAsk\n",
    "    df['OIR'] = (df.bidSize0 - df.askSize0)/(df.bidSize0 + df.askSize0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual time features â€” can consider adding more to the lags list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T02:53:53.773519Z",
     "start_time": "2019-09-19T02:53:53.767518Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_manual_time_features(df):\n",
    "    lags = [*np.arange(1,10), *np.arange(10,100,10), *np.arange(100,1000,100)]\n",
    "    def addTimeFeatures(i):\n",
    "        df['daskRate' + str(i)] = df.askRate0.diff(i)\n",
    "        df['dbidRate' + str(i)] = df.bidRate0.diff(i)\n",
    "    for i in lags:\n",
    "        addTimeFeatures(i)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = compute_cross_sectional(df)\n",
    "# df = add_time_features(df)\n",
    "# df = add_manual_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T03:02:19.677474Z",
     "start_time": "2019-09-18T03:02:14.847627Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.to_feather('intermediate.file')\n",
    "# df = pd.read_feather('intermediate.file', use_threads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tick chart version with ffill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T07:43:52.527931Z",
     "start_time": "2019-09-19T07:43:52.193845Z"
    }
   },
   "outputs": [],
   "source": [
    "# midrate version\n",
    "df['time'] = pd.date_range(start='1/1/1970', periods=2999999, freq='T')\n",
    "df.set_index('time', inplace=True)\n",
    "df_mid = df.midRate.resample('15Min').ohlc()\n",
    "df_mid['vol'] = df.bidAskVol.resample('15Min').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T03:06:35.616471Z",
     "start_time": "2019-09-18T03:02:24.330407Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 5 min\n",
    "df_mid_ta = ta.add_all_ta_features(df_mid, \"open\", \"high\", \"low\", \"close\", \"vol\", fillna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T03:15:55.257617Z",
     "start_time": "2019-09-19T03:15:54.716303Z"
    }
   },
   "outputs": [],
   "source": [
    "# dump(df_mid_ta, 'df_mid_ta.joblib')\n",
    "df_mid_ta = load('df_mid_ta.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T03:07:17.706451Z",
     "start_time": "2019-09-18T03:06:35.912455Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 30s\n",
    "new_df = df.join(df_mid_ta).ffill().astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T08:55:15.867149Z",
     "start_time": "2019-09-19T08:55:01.350433Z"
    }
   },
   "outputs": [],
   "source": [
    "# dump(new_df, 'new_df.joblib')\n",
    "new_df = load('new_df.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:01:56.981404Z",
     "start_time": "2019-09-19T10:01:56.762367Z"
    }
   },
   "outputs": [],
   "source": [
    "# # takes 40s\n",
    "# # undropped\n",
    "# X = new_df.drop('y', axis=1).values\n",
    "# y = new_df.y.values\n",
    "\n",
    "# # standardise\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X)\n",
    "# X_scaled = scaler.transform(X)\n",
    "\n",
    "# # pca takes 1 min\n",
    "# pca = PCA(n_components=50)\n",
    "# X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T08:27:49.777380Z",
     "start_time": "2019-09-19T08:27:49.773376Z"
    }
   },
   "outputs": [],
   "source": [
    "# dump(scaler, 'scaler_drop.joblib')\n",
    "# dump(pca, 'pca_drop.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:06:25.306429Z",
     "start_time": "2019-09-19T10:06:25.290425Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_limited_features(df):\n",
    "    df['midRate'] = (df.askRate0 + df.bidRate0) / 2\n",
    "    df['totalBidVol1'] = df.bidSize0 + df.bidSize1\n",
    "    df['totalAskVol1'] = df.askSize0 + df.askSize1\n",
    "    df['bidAskVol'] = df.askSize0 + df.bidSize0\n",
    "    for i in range(2,5):\n",
    "        df['totalBidVol' + str(i)] = df['totalBidVol' + str(i-1)] + df['bidSize' + str(i)]\n",
    "        df['totalAskVol' + str(i)] = df['totalAskVol' + str(i-1)] + df['askSize' + str(i)]    \n",
    "    df['bidAskRatio4'] = df['totalBidVol' + str(4)] / df['totalAskVol' + str(4)]\n",
    "    df['OIR'] = (df.bidSize0 - df.askSize0)/(df.bidSize0 + df.askSize0)\n",
    "    def addTimeFeatures(i):\n",
    "        df['daskRate' + str(i)] = df.askRate0.diff(i)\n",
    "        df['dbidRate' + str(i)] = df.bidRate0.diff(i)\n",
    "    for i in range(6,11):\n",
    "        addTimeFeatures(i)\n",
    "    df['time'] = pd.date_range(start='1/1/1970', periods=2999999, freq='T')\n",
    "    df.set_index('time', inplace=True)\n",
    "    df_mid = df.midRate.resample('15Min').ohlc()\n",
    "    df_mid['vol'] = df.bidAskVol.resample('15Min').mean()\n",
    "    df_mid['volume_adi'] = ta.volume.acc_dist_index(df_mid.high, df_mid.low, df_mid.close, df_mid.vol, fillna=True)\n",
    "    df_mid['others_dlr'] = ta.others.daily_log_return(df_mid.close, fillna=True)\n",
    "    df = df.join(df_mid[['volume_adi', 'others_dlr']]).ffill().astype('float32')\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:06:32.281786Z",
     "start_time": "2019-09-19T10:06:27.319752Z"
    }
   },
   "outputs": [],
   "source": [
    "sparkle = create_limited_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:06:33.038254Z",
     "start_time": "2019-09-19T10:06:33.033251Z"
    }
   },
   "outputs": [],
   "source": [
    "rlcv = RollingWindowSplit(n_splits=5, compatible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:06:36.117222Z",
     "start_time": "2019-09-19T10:06:34.664203Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 40s\n",
    "# undropped\n",
    "X = sparkle.drop(sparkle.columns[:71], axis=1).values\n",
    "y = df.y.values\n",
    "\n",
    "# standardise\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:06:47.369837Z",
     "start_time": "2019-09-19T10:06:36.473844Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 11s on limited variables, 1 min on pca variables, 16m21s on 232 non-pca variables\n",
    "lasso = LassoLarsCV(cv=rlcv, n_jobs=-1).fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually the lasso above has seen the entire dataset...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:07:11.254439Z",
     "start_time": "2019-09-19T10:07:11.242441Z"
    }
   },
   "outputs": [],
   "source": [
    "def rlcvscore(model):\n",
    "    cvtrain, cvvalid, cvvalidsig = [], [], []\n",
    "    for inc, (train_index, valid_index) in enumerate(rlcv.split(X_scaled), 1):\n",
    "        x_train, x_valid = X_scaled[train_index], X_scaled[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        cvtrain.append(model.score(x_train, y_train))\n",
    "        cvvalid.append(model.score(x_valid, y_valid))\n",
    "        sigmoid = (1/(1+np.exp(-0.22*model.predict(x_valid)))-0.5)*20  \n",
    "        cvvalidsig.append(r2d2(y_valid, sigmoid))\n",
    "    print(f'{np.array(cvtrain).round(4)}')\n",
    "    print(f'{np.array(cvvalid).round(4)}')\n",
    "    print(f'{np.array(cvvalidsig).round(4)}')\n",
    "    print(f'{np.mean(cvtrain):.4f}, {np.mean(cvvalid):.4f}, {np.mean(cvvalidsig):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:07:13.168337Z",
     "start_time": "2019-09-19T10:07:11.649324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0604 0.061  0.0628 0.0589 0.0653]\n",
      "[0.061  0.0628 0.0589 0.0653 0.066 ]\n",
      "[0.0608 0.0626 0.058  0.0652 0.0653]\n",
      "0.0617, 0.0628, 0.0624\n"
     ]
    }
   ],
   "source": [
    "rlcvscore(lasso) # limited variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:07:14.742567Z",
     "start_time": "2019-09-19T10:07:14.729530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lasso_limited.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(lasso, 'lasso_limited.joblib')\n",
    "# lasso = load('lasso_limited.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:55:37.150506Z",
     "start_time": "2019-09-19T09:55:37.058535Z"
    }
   },
   "outputs": [],
   "source": [
    "for inc, (train_index, valid_index) in enumerate(rlcv.split(X_scaled),1):\n",
    "    x_train, x_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    print(\"TRAIN:\", (train_index[0], train_index[-1]),\n",
    "          \"VALID:\", (valid_index[0], valid_index[-1]),\n",
    "          \"SIZES:\", (len(x_train), len(x_valid)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:58:29.694653Z",
     "start_time": "2019-09-19T09:58:29.690653Z"
    }
   },
   "outputs": [],
   "source": [
    "params_grid = {'max_depth': np.arange(10),\n",
    "               'min_samples_split': 2*10**np.arange(5),\n",
    "               'min_samples_leaf': 2*10**np.arange(5)\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:58:32.120288Z",
     "start_time": "2019-09-19T09:58:32.115287Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=30, n_jobs=3, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:58:32.691811Z",
     "start_time": "2019-09-19T09:58:32.686834Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_random = RandomizedSearchCV(rf, params_grid, n_iter=30, n_jobs=3, cv=rlcv, random_state=41, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T10:00:00.932441Z",
     "start_time": "2019-09-19T09:58:34.037439Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_random.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:47:46.631224Z",
     "start_time": "2019-09-19T09:47:46.626222Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=30, max_depth=3, min_samples_split=10, min_samples_leaf=20000,\n",
    "                                 max_features='auto', n_jobs=-1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:47:54.638369Z",
     "start_time": "2019-09-19T09:47:47.984588Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_model.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:47:58.697443Z",
     "start_time": "2019-09-19T09:47:54.641049Z"
    }
   },
   "outputs": [],
   "source": [
    "rlcvscore(rf_model) # realistic cv, max_depth 3, split 10, leaf 10000\n",
    "# [0.0354 0.0389 0.0413 0.0334 0.041 ]\n",
    "# [0.0389 0.0413 0.0334 0.041  0.0275]\n",
    "# [0.0366 0.0392 0.0293 0.0369 0.0202]\n",
    "# 0.0380, 0.0364, 0.0324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T07:12:36.386453Z",
     "start_time": "2019-09-19T07:12:36.353451Z"
    }
   },
   "outputs": [],
   "source": [
    "# for blending validation\n",
    "def get_preds():\n",
    "    # undropped\n",
    "    X = new_df.drop('y', axis=1).values\n",
    "    y = new_df.y.values\n",
    "    scaler = load('scaler.joblib')\n",
    "    X_scaled = scaler.transform(X)\n",
    "    pca = load('pca.joblib')\n",
    "    X_pca = pca.transform(X_scaled)\n",
    "    lasso = load('lassocv.joblib')\n",
    "    \n",
    "    lasso_pred_train, lasso_pred_valid, lasso_pred_validsig = [], [], []\n",
    "    for inc, (train_index, valid_index) in enumerate(rlcv.split(X_pca), 1):\n",
    "        x_train, x_valid = X_pca[train_index], X_pca[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        # obtain lasso predictions\n",
    "        lasso_pred_train.append(lasso.predict(x_train))\n",
    "        lasso_pred_valid.append(lasso.predict(x_valid))\n",
    "        lasso_pred_validsig.append((1/(1+np.exp(-0.22*lasso.predict(x_valid)))-0.5)*20)\n",
    "    return lasso_pred_train, lasso_pred_valid, lasso_pred_validsig    \n",
    "    \n",
    "def get_dropped_preds():\n",
    "    # dropped\n",
    "    X = new_df.drop([*askRateList, *askSizeList, *bidRateList, *bidSizeList, 'y'], axis=1).values\n",
    "    y = new_df.y.values\n",
    "    scaler_drop = load('scaler_drop.joblib')\n",
    "    X_scaled = scaler_drop.transform(X)\n",
    "    pca_drop = load('pca_drop.joblib')\n",
    "    X_pca = pca_drop.transform(X_scaled)\n",
    "#     lasso_drop = load('lassocv_drop.joblib')\n",
    "    lasso_drop = load('lassocv.joblib') # instead use lasso\n",
    "    \n",
    "    y_trainer, y_valider = [], []\n",
    "    lasso_dpred_train, lasso_dpred_valid, lasso_dpred_validsig = [], [], []\n",
    "    for inc, (train_index, valid_index) in enumerate(rlcv.split(X_pca), 1):\n",
    "        x_train, x_valid = X_pca[train_index], X_pca[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]  \n",
    "        # obtain lasso_drop predictions\n",
    "        lasso_dpred_train.append(lasso_drop.predict(x_train))\n",
    "        lasso_dpred_valid.append(lasso_drop.predict(x_valid))\n",
    "        lasso_dpred_validsig.append((1/(1+np.exp(-0.22*lasso_drop.predict(x_valid)))-0.5)*20)\n",
    "        y_trainer.append(y_train)\n",
    "        y_valider.append(y_valid)\n",
    "    return lasso_dpred_train, lasso_dpred_valid, lasso_dpred_validsig, y_trainer, y_valider    \n",
    "    \n",
    "def get_blended_scores():\n",
    "    # average predictions\n",
    "    cvtrain, cvvalid, cvvalidsig = [], [], []\n",
    "    for i in range(5):\n",
    "        train = r2d2(y_trainer[i], (np.array(lasso_dpred_train[i]) + np.array(lasso_pred_train[i]))/2)\n",
    "        valid = r2d2(y_valider[i], (np.array(lasso_dpred_valid[i]) + np.array(lasso_pred_valid[i]))/2)\n",
    "        sigmoid_valid = r2d2(y_valider[i], (np.array(lasso_dpred_validsig[i]) + np.array(lasso_pred_validsig[i]))/2)\n",
    "        cvtrain.append(train)\n",
    "        cvvalid.append(valid)\n",
    "        cvvalidsig.append(sigmoid_valid)   \n",
    "    print(f'{np.array(cvtrain).round(4)}')\n",
    "    print(f'{np.array(cvvalid).round(4)}')\n",
    "    print(f'{np.array(cvvalidsig).round(4)}')\n",
    "    print(f'{np.mean(cvtrain):.4f}, {np.mean(cvvalid):.4f}, {np.mean(cvvalidsig):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T07:10:17.118294Z",
     "start_time": "2019-09-19T07:09:41.953425Z"
    }
   },
   "outputs": [],
   "source": [
    "lasso_pred_train, lasso_pred_valid, lasso_pred_validsig = get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T07:12:52.920189Z",
     "start_time": "2019-09-19T07:12:40.567184Z"
    }
   },
   "outputs": [],
   "source": [
    "lasso_dpred_train, lasso_dpred_valid, lasso_dpred_validsig, y_trainer, y_valider = get_dropped_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T07:12:53.360183Z",
     "start_time": "2019-09-19T07:12:52.924186Z"
    }
   },
   "outputs": [],
   "source": [
    "get_blended_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:34:27.199966Z",
     "start_time": "2019-09-13T08:34:27.194964Z"
    }
   },
   "outputs": [],
   "source": [
    "dep_var = 'y'\n",
    "procs = [FillMissing, Normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:34:46.357867Z",
     "start_time": "2019-09-13T08:34:29.653867Z"
    }
   },
   "outputs": [],
   "source": [
    "path = f'D:\\Coding\\XTX Forecasting Challenge'\n",
    "data = TabularDataBunch.from_df(path = path, df = df[:int(5e5)], dep_var = 'y', procs=procs,\n",
    "                                 valid_idx = list(range(int(4e5),int(5e5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:26:47.618579Z",
     "start_time": "2019-09-13T08:26:17.525620Z"
    }
   },
   "outputs": [],
   "source": [
    "data.show_batch(rows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:27:39.484687Z",
     "start_time": "2019-09-13T06:27:21.410885Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = (TabularList.from_df(df[:int(5e5)], cont_names=df.columns, procs=procs)\n",
    "#                            .split_by_idx(list(range(int(0.8*5e5),int(5e5))))\n",
    "#                            .label_from_df(cols=dep_var, label_cls=FloatList)\n",
    "#                            .databunch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:50:44.246393Z",
     "start_time": "2019-09-13T08:50:44.024206Z"
    }
   },
   "outputs": [],
   "source": [
    "learn = tabular_learner(data, layers=[500,200], metrics=r2_score, ps=[0.001,0.01], emb_drop=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:50:45.534954Z",
     "start_time": "2019-09-13T08:50:45.527952Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:37:03.605733Z",
     "start_time": "2019-09-13T08:36:35.900238Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.lr_find(end_lr=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:37:04.843812Z",
     "start_time": "2019-09-13T08:37:03.608733Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model above has already diverged, we will restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T09:03:36.738452Z",
     "start_time": "2019-09-13T08:50:49.724470Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, 1e-4, wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:41:02.673928Z",
     "start_time": "2019-09-13T08:41:01.868908Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T09:04:03.690828Z",
     "start_time": "2019-09-13T09:04:03.447302Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.save('new_fastai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T09:04:07.023062Z",
     "start_time": "2019-09-13T09:04:06.385055Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T09:05:00.282946Z",
     "start_time": "2019-09-13T09:05:00.098945Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.predict(df.iloc[int(8.1e5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T09:05:02.937494Z",
     "start_time": "2019-09-13T09:05:02.932488Z"
    }
   },
   "outputs": [],
   "source": [
    "df.y.iloc[int(8.1e5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T09:05:38.921082Z",
     "start_time": "2019-09-13T09:05:08.529113Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = learn.get_preds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T02:54:02.574730Z",
     "start_time": "2019-09-19T02:54:02.569726Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_next_data_as_numpy_array(iteration):\n",
    "    return df.iloc[iteration][:60].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T02:54:03.177515Z",
     "start_time": "2019-09-19T02:54:03.171515Z"
    }
   },
   "outputs": [],
   "source": [
    "def append_to_df(massive_df, row):\n",
    "    try: row.index = [massive_df.index[-1] + timedelta(minutes=1)]\n",
    "    except: row.index = [datetime(1970,1,1)]\n",
    "    return massive_df.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T02:54:03.457891Z",
     "start_time": "2019-09-19T02:54:03.441892Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_resample_features(massive_df, resampled_df):\n",
    "    leftovers = len(massive_df) % 15\n",
    "    a = pd.DataFrame()\n",
    "    def pad_history():\n",
    "        full_resampled = resampled_df.append(df_mid)\n",
    "        a = pd.DataFrame([full_resampled.iloc[0] for j in range(30+1-len(full_resampled))])\n",
    "        a = a.append(full_resampled)\n",
    "        a.index = pd.date_range(start=df_mid.index[-1], periods=len(a), freq='-15Min').sort_values()\n",
    "        df_mid_ta = ta.add_all_ta_features(a, \"open\", \"high\", \"low\", \"close\", \"vol\", fillna=True)\n",
    "        return df_mid_ta\n",
    "    if leftovers == 0:\n",
    "        df_mid = massive_df.tail(15).midRate.resample('15Min').ohlc()\n",
    "        df_mid['vol'] = massive_df.tail(15).bidAskVol.resample('15Min').mean()\n",
    "        df_mid_ta = pad_history()\n",
    "        resampled_df = resampled_df.append(df_mid_ta)\n",
    "    else:\n",
    "        df_mid = massive_df.tail(leftovers).midRate.resample('15Min').ohlc()\n",
    "        df_mid['vol'] = massive_df.tail(leftovers).bidAskVol.resample('15Min').mean()\n",
    "        df_mid_ta = pad_history()\n",
    "    massive_df.update(df_mid_ta)\n",
    "    massive_df = massive_df.ffill().astype('float32')\n",
    "    return massive_df, resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T02:54:04.963409Z",
     "start_time": "2019-09-19T02:54:04.958406Z"
    }
   },
   "outputs": [],
   "source": [
    "massive_df, resampled_df = pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T02:54:17.387314Z",
     "start_time": "2019-09-19T02:54:05.659136Z"
    }
   },
   "outputs": [],
   "source": [
    "for iteration in range(30):\n",
    "    base_row = get_next_data_as_numpy_array(len(massive_df))\n",
    "    row = compute_cross_sectional(base_row)\n",
    "    massive_df = append_to_df(massive_df, row)\n",
    "    massive_df, resampled_df = add_resample_features(massive_df, resampled_df)\n",
    "    massive_df = add_time_features(massive_df)\n",
    "    massive_df = add_manual_time_features(massive_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T02:54:51.264489Z",
     "start_time": "2019-09-19T02:54:50.889483Z"
    }
   },
   "outputs": [],
   "source": [
    "massive_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
